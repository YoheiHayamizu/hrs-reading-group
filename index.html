<!doctype html>
<html lang="zh-CN">
    <head>
        <meta charset="ansi">
	<meta name="description" content="this is a website">
        <meta name="keywords" content="website,html,css">

        <title>ReadingGroup</title>

        <link rel="stylesheet" style="text/css" href="index.css">

    </head>
    <body>
        <div id="head">
            <div class="logo_title">
                <h1><font face="verdana" color="#006000">Human-Robot System (HRS) Reading Group</font></h1>
                <h2><font face="verdana" color="#006000">Good papers &amp; Interesting ideas &amp; Delicisous pizzas!</font></h2>
          </div>

            <!--<div class="naviBar">
                <ul>
                    <li><a href=""><font face="verdana" color="green">首页</font></a></li>
                    <li><a href=""><font face="verdana" color="green">闲言碎语</font></a></li>
                    <li><a href=""><font face="verdana" color="green">我是谁</font></a></li>
                </ul>
            </div>-->
            <div class="clearfloat"></div>
        </div>

        <div id="wrapper">
            <div class="main">

<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><font face="verdana" color="red" size="5">Upcomming!</font></h3>
						<h3><a href="https://arxiv.org/pdf/1707.06347.pdf"><font face="verdana" color="black" size="3">Proximal Policy Optimization Algorithms <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Authors: John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Leader: Nicholas A Abate</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Venue: Online</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Date: 12-1 p.m. June 4th, 2020</font></p>
                        <p class="item_info"><a href="https://binghamton.zoom.us/j/3929288199"><font face="verdana" color="red" size="3">Join Zoom Meeting: https://binghamton.zoom.us/j/3929288199</font></a></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: We propose a new family of policy gradient methods for reinforcement learning, which al-
ternate between sampling data through interaction with the environment, and optimizing a
“surrogate” objective function using stochastic gradient ascent. Whereas standard policy gra-
dient methods perform one gradient update per data sample, we propose a novel objective
function that enables multiple epochs of minibatch updates. The new methods, which we call
proximal policy optimization (PPO), have some of the benefits of trust region policy optimiza-
tion (TRPO), but they are much simpler to implement, more general, and have better sample
complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-
ing simulated robotic locomotion and Atari game playing, and we show that PPO outperforms
other online policy gradient methods, and overall strikes a favorable balance between sample
complexity, simplicity, and wall-time.</font></p>
                    </div>
                </div>


            	<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><font face="verdana" color="red" size="5">Upcomming!</font></h3>
						<h3><a href="http://www.roboticsproceedings.org/rss15/p77.pdf"><font face="verdana" color="black" size="3">Commonsense Reasoning and Knowledge
Acquisition to Guide Deep Learning on Robots <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Authors: Tiago Mota and Mohan Sridharan</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Leader: Saeid Amiri</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Venue: Online</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Date: 12-1 p.m. May 28th, 2020</font></p>
                        <p class="item_info"><a href="https://binghamton.zoom.us/j/3929288199"><font face="verdana" color="red" size="3">Join Zoom Meeting: https://binghamton.zoom.us/j/3929288199</font></a></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Algorithms based on deep network models are being
used for many pattern recognition and decision-making tasks in
robotics and AI. Training these models requires a large labeled
dataset and considerable computational resources, which are not
readily available in many domains. Also, it is difficult to under-
stand the internal representations and reasoning mechanisms of
these models. The architecture described in this paper attempts
to address these limitations by drawing inspiration from research
in cognitive systems. It uses non-monotonic logical reasoning
with incomplete commonsense domain knowledge, and inductive
learning of previously unknown constraints on the domain’s
states, to guide the construction of deep network models based
on a small number of relevant training examples. As a motivating
example, we consider a robot reasoning about the stability
and partial occlusion of configurations of objects in simulated
images. Experimental results indicate that in comparison with
an architecture based just on deep networks, our architecture
improves reliability, and reduces the sample complexity and time
complexity of training deep networks.</font></p>
                    </div>
                </div>


                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://dl.acm.org/doi/pdf/10.1145/3313831.3376523?fbclid=IwAR1ep8YTJS3GIiTuu-nZJTSzADLn2vF-1TG5Cp6InWnhY6dTH8ykDzzMeMY"><font face="verdana" color="black" size="3">RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Authors: Ryo Suzuki, Hooman Hedayati, Clement Zheng, James Bohn, Daniel Szafir, Ellen Yi-Luen Do, Mark D. Gross, Daniel Leithinger</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Leader: Kishan D Chandan</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Venue: Online</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Date: 12-1 p.m. May 21st, 2020</font></p>
                        <p class="item_info"><a href="https://binghamton.zoom.us/j/3929288199"><font face="verdana" color="black" size="3">Join Zoom Meeting: https://binghamton.zoom.us/j/3929288199</font></a></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts.  These robots drive beneath  a  piece  of  furniture  to  lift,  move  and  place  it.  By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body;  just as in the real world.  When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content.  We describe the hardware and software  implementation,  applications  in  virtual  tours  and architectural design and interaction techniques.</font></p>
                    </div>
                </div>


                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <!--<h3><font face="verdana" color="red" size="5">Upcomming!</font></h3>-->
						<h3><a href="https://arxiv.org/abs/1707.06203"><font face="verdana" color="black" size="3">Imagination-Augmented Agents for Deep Reinforcement Learning <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Authors: Théophane Weber, Sébastien Racanière, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Leader: Yan Ding</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Venue: Online</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Date: 12-1 p.m. May 7th, 2020</font></p>
                        <p class="item_info"><a href="https://binghamton.zoom.us/j/3929288199"><font face="verdana" color="black" size="2.5">Join Zoom Meeting: https://binghamton.zoom.us/j/3929288199</font></a></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.</font></p>
                    </div>
                </div>

                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
						<h3><a href="https://arxiv.org/pdf/1912.06602.pdf"><font face="verdana" color="black" size="3">That and There: Judging the Intent of Pointing Actions with Robotic Arms <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Authors: Malihe Alikhani, Baber Khalid, Rahul Shome, Chaitanya MitashKostas Bekris, Matthew Stone</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Leader: Yan Ding</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Venue: Online</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Date: 12-1 p.m. Apr. 23th, 2020</font></p>
                        <p class="item_info"><a href="https://binghamton.zoom.us/j/3929288199"><font face="verdana" color="black" size="2.5">Join Zoom Meeting: https://binghamton.zoom.us/j/3929288199</font></a></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Collaborative robotics requires effective communication between a robot and a human partner. This work proposes a set of interpretive principles for how a robotic arm can use pointing actions to communicate task information to people by extending existing models from the related literature. These principles are evaluated through studies where English-speaking human subjects view animations of simulated robots instruct-ing pick-and-place tasks. The evaluation distinguishes two classes of pointing actions that arise in pick-and-place tasks: referential pointing (identifying objects)and locating pointing (identifying locations). The studyindicates that human subjects show greater flexibility ininterpreting the intent of referential pointing comparedto locating pointing, which needs to be more deliberate.The results also demonstrate the effects of variation inthe environment and task context on the interpretation of pointing. Our corpus, experiments and design principles advance models of context, common sense reason-ing and communication in embodied communication.</font></p>
                    </div>
                </div>

                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
						<h3><a href="https://arxiv.org/abs/1912.02241"><font face="verdana" color="black" size="3">Learning from Interventions using Hierarchical Policies for Safety Learning <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Yan Ding  Venue:N09 Date:12-1 p.m. Feb. 20th, 2020</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Learning from Demonstrations (LfD) via Behavior Cloning (BC) works well on multiple complex tasks. However, a limitation of the typical LfD approach is that it requires expert demonstrations for all scenarios, including those in which the algorithm is already well-trained. The recently proposed Learning from Interventions (LfI) overcomes this limitation by using an expert overseer. The expert overseer only intervenes when it suspects that an unsafe action is about to be taken. Although LfI significantly improves over LfD, the state-of-the-art LfI fails to account for delay caused by the expert's reaction time and only learns short-term behavior. We address these limitations by 1) interpolating the expert's interventions back in time, and 2) by splitting the policy into two hierarchical levels, one that generates sub-goals for the future and another that generates actions to reach those desired sub-goals. This sub-goal prediction forces the algorithm to learn long-term behavior while also being robust to the expert's reaction time. Our experiments show that LfI using sub-goals in a hierarchical policy framework trains faster and achieves better asymptotic performance than typical LfD.</font></p>
                    </div>
                </div>

               <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1903.00640.pdf"><font face="verdana" color="black" size="3">Deep Imitation Learning for Autonomous Driving in Generic Urban Scenarios with Enhanced Safety <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Yan Ding,  Venue:P03, Date:12-1 p.m. Oct. 23th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: The decision and planning system for autonomousdriving in urban environments is hard to design. Most currentmethods  manually  design  the  driving  policy,  which  can  be  ex-pensive to develop and maintain at scale. Instead, with imitation learning  we  only  need  to  collect  data  and  the  computer  willlearn  and  improve  the  driving  policy  automatically.  However, existing imitation learning methods for autonomous driving arehardly performing well for complex urban scenarios. Moreover,the   safety   is   not   guaranteed   when   we   use   a   deep   neural network policy. In this paper, we proposed a framework to learnthe  driving  policy  in  urban  scenarios  efficiently  given  offline connected  driving  data,  with  a  safety  controller  incorporatedto guarantee safety at test time. The experiments show that ourmethod  can  achieve  high  performance  in  realistic  simulationsof  urban  driving  scenarios.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://ieeexplore.ieee.org/document/8593700"><font face="verdana" color="black" size="3">Robot Programming Through Augmented Trajectories (IROS, 2019) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Kishan Chandan,  Venue:P03 Date:12-1 p.m., Oct. 16th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: This paper presents a future-focused approach for robot programming based on augmented trajectories. Using a mixed reality head-mounted display (Microsoft Hololens) and a 7-DOF robot arm, we designed an augmented reality (AR) robotic interface with four interactive functions to ease the robot programming task: 1) Trajectory specification. 2) Virtual previews of robot motion. 3) Visualization of robot parameters. 4) Online reprogramming during simulation and execution. We validate our AR-robot teaching interface by comparing it with a kinesthetic teaching interface in two different scenarios as part of a pilot study: creation of contact surface path and free space path. Furthermore, we present an industrial case study that illustrates our AR manufacturing paradigm by interacting with a 7-DOF robot arm to reduce wrinkles during the pleating step of the carbon-fiber-reinforcement-polymer vacuum bagging process in a simulated scenario.</font></p>
                    </div>
                </div>

               <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1805.07830.pdf"><font face="verdana" color="black" size="3">Learning to Teach in Cooperative Multiagent Reinforcement Learning (AAAI, 2019) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenters:Shiqi Zhang and Saeid Amiri,  Venue:G11 Date:12-1 p.m., Apr. 22th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Collective human knowledge has clearly benefited from thefact that innovations by individuals are taught to others throughcommunication. Similar to human social groups, agents indistributed learning systems would likely benefit from com-munication to share knowledge and teach skills. The problemof teaching to improve agent learning has been investigatedby prior works, but these approaches make assumptions thatprevent application of teaching to general multiagent prob-lems, or require domain expertise for problems they can applyto. This learning to teach problem has inherent complexitiesrelated to measuring long-term impacts of teaching that com-pound the standard multiagent coordination challenges. Incontrast to existing works, this paper presents the first gen-eral framework and algorithm for intelligent agents to learn toteach in a multiagent environment. Our algorithm, Learningto Coordinate and Teach Reinforcement (LeCTR), addressespeer-to-peer teaching in cooperative multiagent reinforcementlearning. Each agent in our approach learns both when andwhat to advise, then uses the received advice to improve locallearning. Importantly, these roles are not fixed; these agentslearn to assume the role of student and/or teacher at the ap-propriate moments, requesting and providing advice in orderto improve teamwide performance and learning. Empiricalcomparisons against state-of-the-art teaching methods showthat our teaching agents not only learn significantly faster, butalso learn to coordinate in tasks where existing methods fail.</font></p>
                    </div>
                </div>

                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1903.02020.pdf"><font face="verdana" color="black" size="3">Using Natural Language for Reward Shaping in Reinforcement Learning <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Saeid Amiri,  Venue:G11 Date:12-1 p.m., Apr. 15th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Recent  reinforcement  learning  (RL)  approacheshave  shown  strong  performance  in  complex  do-mains  such  as  Atari  games,  but  are  often  highlysample  inefficient.A  common  approach  to  re-duce  interaction  time  with  the  environment  is  touse  reward  shaping,  which  involves  carefully  de-signing reward functions that provide the agent in-termediate rewards for progress towards the goal.However, designing appropriate shaping rewards isknown to be difficult as well as time-consuming. Inthis work, we address this problem by using naturallanguage  instructions  to  perform  reward  shaping.We propose the LanguagE-Action Reward Network(LEARN),  a  framework  that  maps  free-form  nat-ural language instructions to intermediate rewardsbased on actions taken by the agent.  These inter-mediate language-based rewards can seamlessly beintegrated  into  any  standard  reinforcement  learn-ing algorithm.  We experiment with Montezuma’sRevenge from the Atari Learning Environment,  apopular  benchmark  in  RL.  Our  experiments  on  adiverse  set  of  15  tasks  demonstrate  that,  for  thesame number of interactions with the environment,language-based rewards lead to successful comple-tion of the task 60% more often on average, com-pared to learning without language.</font></p>
                    </div>
                </div>

                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/abs/1709.07174"><font face="verdana" color="black" size="3">Agile Autonomous Driving using End-to-End Deep Imitation Learning (RSS, 2018) <font face="verdana" color="black" size="0.5">(click it!)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Roger Correia,  Venue:P03 Date:12-1 p.m., Apr. 1st, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="http://lis.csail.mit.edu/pubs/kim-aaai19.pdf"><font face="verdana" color="black" size="3">Adversarial Actor-Critic Method for Task and Motion Planning Problems Using Planning Experience (AAAI, 2019) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Roger Correia  Venue:G11 Date:12-1 p.m. Mar. 25th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: We propose  an  actor-critic  algorithm  that  uses  past  plan-ning  experience  to  improve  the  efficiency  of  solving  robottask-and-motion  planning  (TAMP)  problems.  TAMP  plan-ners  search  for  goal-achieving  sequences  of  high-level  op-erator  instances  specified  by  both  discrete  and  continuousparameters.  Our  algorithm  learns  a  policy  for  selecting  thecontinuous parameters during search, using a small trainingset generated from the search trees of previously solved in-stances.  We also  introduce a novel  fixed-length  vector rep-resentation for world states with varying numbers of objectswith different shapes, based on a set of key robot configura-tions. We demonstrate experimentally that our method learnsmore efficiently from less data than standard reinforcement-learning approaches and that using a learned policy to guidea planner results in the improvement of planning efficiency.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1604.04562.pdf"><font face="verdana" color="black" size="3">A Network-based End-to-End Trainable Task-oriented Dialogue System (Association for Computational Linguistic, 2017) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Shiqi Zhang  Venue:G11 Date:12-1 p.m. Mar. 11th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Teaching  machines  to  accomplish  tasksby  conversing  naturally  with  humans  ischallenging.   Currently,  developing task-oriented dialogue systems requires creatingmultiple components and typically this in-volves either a large amount of handcraft-ing, or acquiring costly labelled datasetsto solve a statistical learning problem foreach component.   In this work we intro-duce a neural network-based text-in, text-out end-to-end trainable goal-oriented di-alogue system along with a new way ofcollecting dialogue data based on a novelpipe-lined Wizard-of-Oz framework. Thisapproach allows us to develop dialogue sys-tems easily and without making too manyassumptions about the task at hand.  Theresults show that the model can conversewith human subjects naturally whilst help-ing them to accomplish tasks in a restaurantsearch domain.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://dl.acm.org/doi/10.1145/3171221.3171253"><font face="verdana" color="black" size="3">Communicating Robot Motion Intent with Augmented Reality (HRI, 2018) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Kishan Chandan  Venue:G11 Date:12-1 p.m. Mar. 4th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://papers.nips.cc/paper/7299-learning-pipelines-with-limited-data-and-domain-knowledge-a-study-in-parsing-physics-problems.pdf"><font face="verdana" color="black" size="3">Learning Pipelines with Limited Data and Domain Knowledge (NeurIPS, 2018) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Saeid Amiri, Venue:G11 Date:12-1 p.m. Feb. 11st, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software,and with domain knowledge encoded as rules. As a case study, we present sucha system that learns to parse Newtonian physics problems in textbooks.   This system, Nuts&amp;Bolts, learns a pipeline process that incorporates existing code,pre-learned machine learning models, and human engineered rules. It jointly trainsthe entire pipeline to prevent propagation of errors, using a combination of labelledand unlabelled data.  Our approach achieves a good performance on the parsingtask, outperforming the simple pipeline and its variants. Finally, we also show how Nuts&amp;Boltscan be used to achieve improvements on a relation extraction taskand on the end task of answering Newtonian physics problems.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/abs/1805.01954"><font face="verdana" color="black" size="3">Behavioral Cloning from Observation (IJCAI, 2018) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Saeid Amiri, Venue:G11 Date:12-1 p.m. Feb. 11st, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract: Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.</font></p>
                    </div>
                </div>
            </div>

            <div class="side">
                <div class="author_info">
                    <!--<div class="author_Image">
                        <img src="air.jpg" alt="this is a author image">
                    </div>-->

                    <div class="author_descri">
                        <h4><font face="verdana" color="black" size="4">About Reading Group</font></h4>
                        <p align="center">
							<font face="verdana" color="black" size="2.8">The reading group aims to broaden the scope of research for students, especially in the human-robot system, by sharing and discussing some good papers. This meeting will be held every two weeks by <a href="http://robotics.cs.binghamton.edu"> <font face="verdana" color="black" size="2.8"> <U> AIR Group </U> </font> </a> at SUNY Binghamton University. If you want to learn about new research papers, or love to have discussions on interesting research ideas, HRS Reading Group is for you!</font>
					    </p>
                    </div>
                </div>
				
				<div class="top_article">
                    <h3><font face="verdana" color="black" size="4">How to join?</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Contacts: <U>Yan Ding</U></font></li>
						<li><font face="verdana" color="black" size="2.8">Email: <U>yding25@binghamton.edu</U></font>
						<li><font face="verdana" color="black" size="2.8">Homepage: <a href="https://yding25.github.io"><U><font face="verdana" color="black" size="2.8">yding25.github.io</font></U></a> </font></li>
                    </ul>
                </div>
				
                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Recommended Papers</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Paper-1: Proximal Policy Optimization Algorithms</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-2: Imagination-Augmented Agents for Deep Reinforcement Learning</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-3</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-4</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-5</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-6</font></li>
                    </ul>
                </div>

                 <!--<div class="site_info">
                    <p>访客:321548名</p>
                    <p>文章:1000篇</p>
                </div>-->

            </div>

            <div class="clearfloat"></div>
        </div>

        <div id="footer">
            <div class="copyRight">
              <p>2016-2050 CopyRight website Demo Site</p>
            </div>

            <div class="site_link">
                <ul>
                    <li><a href="">关于我们</a></li>
                    <li><a href="">联系我们</a></li>
                    <li><a href="">使用条款</a></li>
                    <li><a href="">意见反馈</a></li>
                <ul>
            </div>
        </div>

    </body>
</html>
