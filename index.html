<!doctype html>
<html lang="zh-CN">
    <head>
        <meta charset="ansi">
        <meta name="description" content="this is a website">
        <meta name="keywords" content="website,html,css">

        <title>ReadingGroup</title>

        <link rel="stylesheet" style="text/css" href="index.css">

    </head>
    <body>
        <div id="head">
            <div class="logo_title">
                <h1><font face="verdana" color="#006000">Reading Group</font></h1>
                <h2><font face="verdana" color="#006000">Sharing and discussing good research papers</font></h2>
            </div>

            <!--<div class="naviBar">
                <ul>
                    <li><a href=""><font face="verdana" color="green">首页</font></a></li>
                    <li><a href=""><font face="verdana" color="green">闲言碎语</font></a></li>
                    <li><a href=""><font face="verdana" color="green">我是谁</font></a></li>
                </ul>
            </div>-->
            <div class="clearfloat"></div>
        </div>

        <div id="wrapper">
            <div class="main">
                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><font face="verdana" color="red" size="5">Upcomming!</font></h3>
						<h3><a href="https://www.google.com/"><font face="verdana" color="black" size="3">Title of the paper <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Name  Venue:P03 Date:1 p.m. January 26th, 2020</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Abstract of the paper</font></p>
                    </div>
                </div>

               <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1903.00640.pdf"><font face="verdana" color="black" size="3">Deep Imitation Learning for Autonomous Driving in Generic Urban Scenarios with Enhanced Safety <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Yan Ding,  Venue:P03, Date:1 p.m. Oct. 23th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">The decision and planning system for autonomousdriving in urban environments is hard to design. Most currentmethods  manually  design  the  driving  policy,  which  can  be  ex-pensive to develop and maintain at scale. Instead, with imitation learning  we  only  need  to  collect  data  and  the  computer  willlearn  and  improve  the  driving  policy  automatically.  However, existing imitation learning methods for autonomous driving arehardly performing well for complex urban scenarios. Moreover,the   safety   is   not   guaranteed   when   we   use   a   deep   neural network policy. In this paper, we proposed a framework to learnthe  driving  policy  in  urban  scenarios  efficiently  given  offline connected  driving  data,  with  a  safety  controller  incorporatedto guarantee safety at test time. The experiments show that ourmethod  can  achieve  high  performance  in  realistic  simulationsof  urban  driving  scenarios.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://ieeexplore.ieee.org/document/8593700"><font face="verdana" color="black" size="3">Robot Programming Through Augmented Trajectories (IROS, 2019) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Kishan Chandan,  Venue:P03 Date:12 p.m., Oct. 16th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">This paper presents a future-focused approach for robot programming based on augmented trajectories. Using a mixed reality head-mounted display (Microsoft Hololens) and a 7-DOF robot arm, we designed an augmented reality (AR) robotic interface with four interactive functions to ease the robot programming task: 1) Trajectory specification. 2) Virtual previews of robot motion. 3) Visualization of robot parameters. 4) Online reprogramming during simulation and execution. We validate our AR-robot teaching interface by comparing it with a kinesthetic teaching interface in two different scenarios as part of a pilot study: creation of contact surface path and free space path. Furthermore, we present an industrial case study that illustrates our AR manufacturing paradigm by interacting with a 7-DOF robot arm to reduce wrinkles during the pleating step of the carbon-fiber-reinforcement-polymer vacuum bagging process in a simulated scenario.</font></p>
                    </div>
                </div>

               <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1805.07830.pdf"><font face="verdana" color="black" size="3">Learning to Teach in Cooperative Multiagent Reinforcement Learning (AAAI, 2019) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenters:Shiqi Zhang and Saeid Amiri,  Venue:G11 Date:12 p.m., Apr. 22th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Collective human knowledge has clearly benefited from thefact that innovations by individuals are taught to others throughcommunication. Similar to human social groups, agents indistributed learning systems would likely benefit from com-munication to share knowledge and teach skills. The problemof teaching to improve agent learning has been investigatedby prior works, but these approaches make assumptions thatprevent application of teaching to general multiagent prob-lems, or require domain expertise for problems they can applyto. This learning to teach problem has inherent complexitiesrelated to measuring long-term impacts of teaching that com-pound the standard multiagent coordination challenges. Incontrast to existing works, this paper presents the first gen-eral framework and algorithm for intelligent agents to learn toteach in a multiagent environment. Our algorithm, Learningto Coordinate and Teach Reinforcement (LeCTR), addressespeer-to-peer teaching in cooperative multiagent reinforcementlearning. Each agent in our approach learns both when andwhat to advise, then uses the received advice to improve locallearning. Importantly, these roles are not fixed; these agentslearn to assume the role of student and/or teacher at the ap-propriate moments, requesting and providing advice in orderto improve teamwide performance and learning. Empiricalcomparisons against state-of-the-art teaching methods showthat our teaching agents not only learn significantly faster, butalso learn to coordinate in tasks where existing methods fail.</font></p>
                    </div>
                </div>

                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1903.02020.pdf"><font face="verdana" color="black" size="3">Using Natural Language for Reward Shaping in Reinforcement Learning <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Saeid Amiri,  Venue:G11 Date:12 p.m., Apr. 15th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Recent  reinforcement  learning  (RL)  approacheshave  shown  strong  performance  in  complex  do-mains  such  as  Atari  games,  but  are  often  highlysample  inefficient.A  common  approach  to  re-duce  interaction  time  with  the  environment  is  touse  reward  shaping,  which  involves  carefully  de-signing reward functions that provide the agent in-termediate rewards for progress towards the goal.However, designing appropriate shaping rewards isknown to be difficult as well as time-consuming. Inthis work, we address this problem by using naturallanguage  instructions  to  perform  reward  shaping.We propose the LanguagE-Action Reward Network(LEARN),  a  framework  that  maps  free-form  nat-ural language instructions to intermediate rewardsbased on actions taken by the agent.  These inter-mediate language-based rewards can seamlessly beintegrated  into  any  standard  reinforcement  learn-ing algorithm.  We experiment with Montezuma’sRevenge from the Atari Learning Environment,  apopular  benchmark  in  RL.  Our  experiments  on  adiverse  set  of  15  tasks  demonstrate  that,  for  thesame number of interactions with the environment,language-based rewards lead to successful comple-tion of the task 60% more often on average, com-pared to learning without language.</font></p>
                    </div>
                </div>

                <div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/abs/1709.07174"><font face="verdana" color="black" size="3">Agile Autonomous Driving using End-to-End Deep Imitation Learning (RSS, 2018) <font face="verdana" color="black" size="0.5">(click it!)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Roger Correia,  Venue:P03 Date:12 p.m., Apr. 1st, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="http://lis.csail.mit.edu/pubs/kim-aaai19.pdf"><font face="verdana" color="black" size="3">Adversarial Actor-Critic Method for Task and Motion Planning Problems Using Planning Experience (AAAI, 2019) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Roger Correia  Venue:G11 Date:12 p.m. Mar. 25th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">We  propose  an  actor-critic  algorithm  that  uses  past  plan-ning  experience  to  improve  the  efficiency  of  solving  robottask-and-motion  planning  (TAMP)  problems.  TAMP  plan-ners  search  for  goal-achieving  sequences  of  high-level  op-erator  instances  specified  by  both  discrete  and  continuousparameters.  Our  algorithm  learns  a  policy  for  selecting  thecontinuous parameters during search, using a small trainingset generated from the search trees of previously solved in-stances.  We also  introduce a novel  fixed-length  vector rep-resentation for world states with varying numbers of objectswith different shapes, based on a set of key robot configura-tions. We demonstrate experimentally that our method learnsmore efficiently from less data than standard reinforcement-learning approaches and that using a learned policy to guidea planner results in the improvement of planning efficiency.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/pdf/1604.04562.pdf"><font face="verdana" color="black" size="3">A Network-based End-to-End Trainable Task-oriented Dialogue System (Association for Computational Linguistic, 2017) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Shiqi Zhang  Venue:G11 Date:12 p.m. Mar. 11th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Teaching  machines  to  accomplish  tasksby  conversing  naturally  with  humans  ischallenging.   Currently,  developing task-oriented dialogue systems requires creatingmultiple components and typically this in-volves either a large amount of handcraft-ing, or acquiring costly labelled datasetsto solve a statistical learning problem foreach component.   In this work we intro-duce a neural network-based text-in, text-out end-to-end trainable goal-oriented di-alogue system along with a new way ofcollecting dialogue data based on a novelpipe-lined Wizard-of-Oz framework. Thisapproach allows us to develop dialogue sys-tems easily and without making too manyassumptions about the task at hand.  Theresults show that the model can conversewith human subjects naturally whilst help-ing them to accomplish tasks in a restaurantsearch domain.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://dl.acm.org/doi/10.1145/3171221.3171253"><font face="verdana" color="black" size="3">Communicating Robot Motion Intent with Augmented Reality (HRI, 2018) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Kishan Chandan  Venue:G11 Date:12 p.m. Mar. 4th, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Humans coordinate teamwork by conveying intent through social cues, such as gestures and gaze behaviors. However, these methods may not be possible for appearance-constrained robots that lack anthropomorphic or zoomorphic features, such as aerial robots. We explore a new design space for communicating robot motion intent by investigating how augmented reality (AR) might mediate human-robot interactions. We develop a series of explicit and implicit designs for visually signaling robot motion intent using AR, which we evaluate in a user study. We found that several of our AR designs significantly improved objective task efficiency over a baseline in which users only received physically-embodied orientation cues. In addition, our designs offer several trade-offs in terms of intent clarity and user perceptions of the robot as a teammate.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://papers.nips.cc/paper/7299-learning-pipelines-with-limited-data-and-domain-knowledge-a-study-in-parsing-physics-problems.pdf"><font face="verdana" color="black" size="3">Learning Pipelines with Limited Data and Domain Knowledge (NeurIPS, 2018) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Saeid Amiri, Venue:G11 Date:12 p.m. Feb. 11st, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software,and with domain knowledge encoded as rules. As a case study, we present sucha system that learns to parse Newtonian physics problems in textbooks.   This system,Nuts&Bolts, learns a pipeline process that incorporates existing code,pre-learned machine learning models, and human engineered rules. It jointly trainsthe entire pipeline to prevent propagation of errors, using a combination of labelledand unlabelled data.  Our approach achieves a good performance on the parsingtask, outperforming the simple pipeline and its variants. Finally, we also show how Nuts&Boltscan be used to achieve improvements on a relation extraction taskand on the end task of answering Newtonian physics problems.</font></p>
                    </div>
                </div>
				
				<div class="item">
                    <div class="item_image">
                        <img src="air.jpg" alt="this is a contentImage">
                    </div>
                    <div class="item_content">
                        <h3><a href="https://arxiv.org/abs/1805.01954"><font face="verdana" color="black" size="3">Behavioral Cloning from Observation (IJCAI, 2018) <font face="verdana" color="black" size="0.5">(click it !)</font> </font></a></h3>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter:Saeid Amiri, Venue:G11 Date:12 p.m. Feb. 11st, 2019</font></p>
                        <p class="item_descri"><font face="verdana" color="black" size="2.5">Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.</font></p>
                    </div>
                </div>
            </div>

            <div class="side">
                <div class="author_info">
                    <div class="author_Image">
                        <img src="air.jpg" alt="this is a author image">
                    </div>

                    <div class="author_descri">
                        <h4><font face="verdana" color="black" size="4">About AIR Group</font></h4>
                        <p align="center">
							<font face="verdana" color="black" size="2.8">We are a group of researchers, at SUNY Binghamton. AIR group's research interests include Artificial Intelligence (AI), Robotics, and Human-Robot Interaction (HRI). Our goal is to develop intelligent mobile robots that are able to interact with people, provide services to people, and learn from this experience, in human-inhabited, collaborative environments.</font>
					    </p>
                    </div>
                </div>

                <div class="top_article">
                    <h3>Recommended Papers</h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Paper-1</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-2</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-3</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-4</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-5</font></li>
                        <li><font face="verdana" color="black" size="2.8">Paper-6</font></li>
                    </ul>
                </div>

                 <!--<div class="site_info">
                    <p>访客:321548名</p>
                    <p>文章:1000篇</p>
                </div>-->

            </div>

            <div class="clearfloat"></div>
        </div>

        <div id="footer">
            <div class="copyRight">
              <p>2016-2050 CopyRight website Demo Site</p>
            </div>

            <div class="site_link">
                <ul>
                    <li><a href="">关于我们</a></li>
                    <li><a href="">联系我们</a></li>
                    <li><a href="">使用条款</a></li>
                    <li><a href="">意见反馈</a></li>
                <ul>
            </div>
        </div>

    </body>
</html>